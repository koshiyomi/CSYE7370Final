{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from StockTrade import StockTradeDiscrete\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from StockTrade import StockTrade\n",
    "import os\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import HER, DDPG, DQN, SAC, TD3\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from stable_baselines3.common.bit_flipping_env import BitFlippingEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.obs_dict_wrapper import ObsDictWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_raw = StockTradeDiscrete()\n",
    "env = Monitor(env_raw, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Num timesteps: 2500\n",
      "Best mean reward: -inf - Last mean reward per episode: -169.38\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -175     |\n",
      "|    exploration rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 9837     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 4000     |\n",
      "----------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -169.38 - Last mean reward per episode: -169.80\n",
      "Num timesteps: 7500\n",
      "Best mean reward: -169.38 - Last mean reward per episode: -168.57\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -164     |\n",
      "|    exploration rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 9847     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 8000     |\n",
      "----------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -168.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -172     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 9684     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total timesteps  | 12000    |\n",
      "----------------------------------\n",
      "Num timesteps: 12500\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -170.00\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -169.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -168     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 9441     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total timesteps  | 16000    |\n",
      "----------------------------------\n",
      "Num timesteps: 17500\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -168.77\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -168.26\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 9208     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total timesteps  | 20000    |\n",
      "----------------------------------\n",
      "Num timesteps: 22500\n",
      "Best mean reward: -168.26 - Last mean reward per episode: -168.00\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 9091     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total timesteps  | 24000    |\n",
      "----------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -168.00 - Last mean reward per episode: -167.80\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 27500\n",
      "Best mean reward: -167.80 - Last mean reward per episode: -167.23\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 9157     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total timesteps  | 28000    |\n",
      "----------------------------------\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -167.23 - Last mean reward per episode: -167.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 9105     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total timesteps  | 32000    |\n",
      "----------------------------------\n",
      "Num timesteps: 32500\n",
      "Best mean reward: -167.23 - Last mean reward per episode: -167.79\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -167.23 - Last mean reward per episode: -167.22\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 9130     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total timesteps  | 36000    |\n",
      "----------------------------------\n",
      "Num timesteps: 37500\n",
      "Best mean reward: -167.22 - Last mean reward per episode: -167.15\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -167.15 - Last mean reward per episode: -166.69\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 9188     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Num timesteps: 42500\n",
      "Best mean reward: -166.69 - Last mean reward per episode: -166.42\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 9244     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total timesteps  | 44000    |\n",
      "----------------------------------\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -166.42 - Last mean reward per episode: -165.97\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 47500\n",
      "Best mean reward: -165.97 - Last mean reward per episode: -165.62\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -164     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 9257     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total timesteps  | 48000    |\n",
      "----------------------------------\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -165.62 - Last mean reward per episode: -165.54\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -157     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 4470     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.117    |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "Num timesteps: 52500\n",
      "Best mean reward: -165.54 - Last mean reward per episode: -161.41\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -161.41 - Last mean reward per episode: -155.75\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -146     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 2317     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.084    |\n",
      "|    n_updates        | 1499     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 57500\n",
      "Best mean reward: -155.75 - Last mean reward per episode: -152.23\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -152.23 - Last mean reward per episode: -147.14\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -136     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1638     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Num timesteps: 62500\n",
      "Best mean reward: -147.14 - Last mean reward per episode: -143.85\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -127     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 1281     |\n",
      "|    time_elapsed     | 49       |\n",
      "|    total timesteps  | 64000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.103    |\n",
      "|    n_updates        | 3499     |\n",
      "----------------------------------\n",
      "Num timesteps: 65000\n",
      "Best mean reward: -143.85 - Last mean reward per episode: -139.36\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 67500\n",
      "Best mean reward: -139.36 - Last mean reward per episode: -136.56\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -120     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 1045     |\n",
      "|    time_elapsed     | 65       |\n",
      "|    total timesteps  | 68000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0462   |\n",
      "|    n_updates        | 4499     |\n",
      "----------------------------------\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -136.56 - Last mean reward per episode: -132.53\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -113     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 930      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total timesteps  | 72000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0769   |\n",
      "|    n_updates        | 5499     |\n",
      "----------------------------------\n",
      "Num timesteps: 72500\n",
      "Best mean reward: -132.53 - Last mean reward per episode: -129.22\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 75000\n",
      "Best mean reward: -129.22 - Last mean reward per episode: -123.26\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -107     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 849      |\n",
      "|    time_elapsed     | 89       |\n",
      "|    total timesteps  | 76000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.077    |\n",
      "|    n_updates        | 6499     |\n",
      "----------------------------------\n",
      "Num timesteps: 77500\n",
      "Best mean reward: -123.26 - Last mean reward per episode: -120.35\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -120.35 - Last mean reward per episode: -115.42\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -102     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00311  |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Num timesteps: 82500\n",
      "Best mean reward: -115.42 - Last mean reward per episode: -111.62\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -97      |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 738      |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total timesteps  | 84000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.112    |\n",
      "|    n_updates        | 8499     |\n",
      "----------------------------------\n",
      "Num timesteps: 85000\n",
      "Best mean reward: -111.62 - Last mean reward per episode: -105.97\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 87500\n",
      "Best mean reward: -105.97 - Last mean reward per episode: -102.68\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -92.6    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 127      |\n",
      "|    total timesteps  | 88000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0304   |\n",
      "|    n_updates        | 9499     |\n",
      "----------------------------------\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -102.68 - Last mean reward per episode: -97.56\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -88.6    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 654      |\n",
      "|    time_elapsed     | 140      |\n",
      "|    total timesteps  | 92000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.121    |\n",
      "|    n_updates        | 10499    |\n",
      "----------------------------------\n",
      "Num timesteps: 92500\n",
      "Best mean reward: -97.56 - Last mean reward per episode: -95.00\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 95000\n",
      "Best mean reward: -95.00 - Last mean reward per episode: -90.03\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -84.9    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 628      |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total timesteps  | 96000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0731   |\n",
      "|    n_updates        | 11499    |\n",
      "----------------------------------\n",
      "Num timesteps: 97500\n",
      "Best mean reward: -90.03 - Last mean reward per episode: -86.62\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -86.62 - Last mean reward per episode: -81.50\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -81.5    |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 604      |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00867  |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = DQN(MlpPolicy, env, verbose=1)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=2500, log_dir=log_dir)\n",
    "timesteps = 100000\n",
    "\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"Stock Trade Game\")\n",
    "plt.show()\n",
    "model.save(\"123Automator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
