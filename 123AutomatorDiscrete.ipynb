{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from StockTrade import StockTradeDiscrete\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "from StockTrade import StockTrade\n",
    "import os\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3 import HER, DDPG, DQN, SAC, TD3\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from stable_baselines3.common.bit_flipping_env import BitFlippingEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.obs_dict_wrapper import ObsDictWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(\"Num timesteps: {}\".format(self.num_timesteps))\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(self.best_mean_reward, mean_reward))\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(\"Saving new best model to {}\".format(self.save_path))\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"tmp/\"\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_raw = StockTradeDiscrete()\n",
    "env = Monitor(env_raw, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Num timesteps: 2500\n",
      "Best mean reward: -inf - Last mean reward per episode: -169.38\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -175     |\n",
      "|    exploration rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 9837     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 4000     |\n",
      "----------------------------------\n",
      "Num timesteps: 5000\n",
      "Best mean reward: -169.38 - Last mean reward per episode: -169.80\n",
      "Num timesteps: 7500\n",
      "Best mean reward: -169.38 - Last mean reward per episode: -168.57\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -164     |\n",
      "|    exploration rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 9847     |\n",
      "|    time_elapsed     | 0        |\n",
      "|    total timesteps  | 8000     |\n",
      "----------------------------------\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -168.75\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -172     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 9684     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total timesteps  | 12000    |\n",
      "----------------------------------\n",
      "Num timesteps: 12500\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -170.00\n",
      "Num timesteps: 15000\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -169.18\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -168     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 9441     |\n",
      "|    time_elapsed     | 1        |\n",
      "|    total timesteps  | 16000    |\n",
      "----------------------------------\n",
      "Num timesteps: 17500\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -168.77\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -168.57 - Last mean reward per episode: -168.26\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -167     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 9208     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total timesteps  | 20000    |\n",
      "----------------------------------\n",
      "Num timesteps: 22500\n",
      "Best mean reward: -168.26 - Last mean reward per episode: -168.00\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 9091     |\n",
      "|    time_elapsed     | 2        |\n",
      "|    total timesteps  | 24000    |\n",
      "----------------------------------\n",
      "Num timesteps: 25000\n",
      "Best mean reward: -168.00 - Last mean reward per episode: -167.80\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 27500\n",
      "Best mean reward: -167.80 - Last mean reward per episode: -167.23\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 9157     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total timesteps  | 28000    |\n",
      "----------------------------------\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -167.23 - Last mean reward per episode: -167.32\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 9105     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total timesteps  | 32000    |\n",
      "----------------------------------\n",
      "Num timesteps: 32500\n",
      "Best mean reward: -167.23 - Last mean reward per episode: -167.79\n",
      "Num timesteps: 35000\n",
      "Best mean reward: -167.23 - Last mean reward per episode: -167.22\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -166     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 9130     |\n",
      "|    time_elapsed     | 3        |\n",
      "|    total timesteps  | 36000    |\n",
      "----------------------------------\n",
      "Num timesteps: 37500\n",
      "Best mean reward: -167.22 - Last mean reward per episode: -167.15\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -167.15 - Last mean reward per episode: -166.69\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -165     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 9188     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Num timesteps: 42500\n",
      "Best mean reward: -166.69 - Last mean reward per episode: -166.42\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -163     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 9244     |\n",
      "|    time_elapsed     | 4        |\n",
      "|    total timesteps  | 44000    |\n",
      "----------------------------------\n",
      "Num timesteps: 45000\n",
      "Best mean reward: -166.42 - Last mean reward per episode: -165.97\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 47500\n",
      "Best mean reward: -165.97 - Last mean reward per episode: -165.62\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -164     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 9257     |\n",
      "|    time_elapsed     | 5        |\n",
      "|    total timesteps  | 48000    |\n",
      "----------------------------------\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -165.62 - Last mean reward per episode: -165.54\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -157     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 4470     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total timesteps  | 52000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.117    |\n",
      "|    n_updates        | 499      |\n",
      "----------------------------------\n",
      "Num timesteps: 52500\n",
      "Best mean reward: -165.54 - Last mean reward per episode: -161.41\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 55000\n",
      "Best mean reward: -161.41 - Last mean reward per episode: -155.75\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -146     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 2317     |\n",
      "|    time_elapsed     | 24       |\n",
      "|    total timesteps  | 56000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.084    |\n",
      "|    n_updates        | 1499     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num timesteps: 57500\n",
      "Best mean reward: -155.75 - Last mean reward per episode: -152.23\n",
      "Saving new best model to tmp/best_model\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -152.23 - Last mean reward per episode: -147.14\n",
      "Saving new best model to tmp/best_model\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1e+03    |\n",
      "|    ep_rew_mean      | -136     |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 1638     |\n",
      "|    time_elapsed     | 36       |\n",
      "|    total timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.13     |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = DQN(MlpPolicy, env, verbose=1)\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=2500, log_dir=log_dir)\n",
    "timesteps = 100000\n",
    "\n",
    "model.learn(total_timesteps=int(timesteps), callback=callback)\n",
    "plot_results([log_dir], timesteps, results_plotter.X_TIMESTEPS, \"Stock Trade Game\")\n",
    "plt.show()\n",
    "model.save(\"123Automator\")\n",
    "\n",
    "obs = env.reset()\n",
    "# while True:\n",
    "#     action, _states = model.predict(obs, deterministic=True)\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     env.render()\n",
    "#     if done:\n",
    "#       obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
